{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import scipy.stats as sstats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Typical Linear Problem\n",
    "\n",
    "Consider the linear problem\n",
    "\n",
    "$$\n",
    "    q = Ax, \\ x\\in\\mathbb{R}^n, \\ q\\in \\mathbb{R}^p, \\ A\\in\\mathbb{R}^{p\\times n}\n",
    "$$\n",
    "\n",
    "where we assume that $p\\leq n$ and $A$ is rank $p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Bayesian connection to Tikhonov regularization\n",
    "\n",
    "Assuming we observe datum $\\tilde{q}$ and that we are using a Gaussian prior ($N(\\bar{x},C_x)$) and Gaussian noise model ($N(0,C_q)$), the statistical Bayesian posterior is given by\n",
    "\n",
    "$$\n",
    "    \\pi^{\\text{post}} \\propto \\exp\\left(-\\frac{1}{2}\\left( \n",
    "    \\underbrace{\\left|\\left|C_q^{-1/2}(q-\\tilde{q})\\right|\\right|_2^2}_{\\text{Data mismatch}} + \n",
    "    \\underbrace{\\left|\\left|C_x^{-1/2}(x-\\bar{x})\\right|\\right|_2^2}_{\\text{Tikhonov regularization}}\n",
    "    \\right)\\right)\n",
    "$$\n",
    "\n",
    "where we have made explicit the connection of the MAP (maximum a posteriori) point of the posterior density with the Tikhnoov regularized solution to a deterministic optimization problem.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Tikhonov_regularization for more information.\n",
    "\n",
    "## Take-aways\n",
    "\n",
    "\n",
    "* The model defines the data mismatch and the prior defines the regularization. \n",
    "\n",
    "\n",
    "* The regularization impacts ***all directions*** of the posterior since we effectively balance the data mismatch with our prior beliefs. This implies that the \"solution\" defined by a MAP point is not necessarily a point that produces the observed datum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent Bayesian and *unregularization*\n",
    "\n",
    "The statistical Bayesian posterior is given by the prior times the likelihood function divided by a normalizing constant.\n",
    "However, the consistent Bayesian posterior is given by the observed density times the prior density divided by the push-forward of the prior density.\n",
    "In this problem, with the same assumptions as above, the push-forward is Gaussian with covariance given by $C_A=AC_xA^\\top$ and mean given by $\\bar{q}=A\\bar{x}$. \n",
    "Thus, the consistent Bayesian posterior is given by\n",
    "\n",
    "$$\n",
    "    \\pi^{\\text{post}} \\sim \\exp\\left(-\\frac{1}{2}\\left( \n",
    "    \\underbrace{\\left|\\left|C_q^{-1/2}(q-\\tilde{q})\\right|\\right|_2^2}_{\\text{Data mismatch}} \n",
    "    + \n",
    "    \\underbrace{\\left|\\left|C_x^{-1/2}(x-\\bar{x})\\right|\\right|_2^2}_{\\text{Tikhonov regularization}}\n",
    "    - \n",
    "    \\underbrace{\\left|\\left|C_A^{-1/2}(q-\\bar{q})\\right|\\right|_2^2}_{\\text{unregularization}}\n",
    "    \\right)\\right)\n",
    "$$\n",
    "\n",
    "where we have categorized the effect of dividing by the push-forward as a source of ***unregularization*** in the MAP point.\n",
    "To see this even more clearly, observe that we can collect and rewrite the regularization terms as\n",
    "\n",
    "$$\n",
    "    \\left|\\left|C_x^{-1/2}(x-\\bar{x})\\right|\\right|_2^2 - \\left|\\left|C_A^{-1/2}(q-\\bar{q})\\right|\\right|_2^2 = (x-\\bar{x})^\\top\\underbrace{\\left(C_x^{-1}-A^\\top(AC_xA^\\top)^{-1}\\right)}_{R}(x-\\bar{x})\n",
    "$$\n",
    "\n",
    "where $R$ is the ***regularization matrix***.\n",
    "\n",
    "## Take-aways\n",
    "\n",
    "\n",
    "* If $A$ is rank $n$ and invertible, then $R=0$. In other words, regularization is ***turned off*** if there is a unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Consider the linear problem where\n",
    "$$\n",
    "    A = [2 \\ -1],\\ \\bar{x}=\\left[\\begin{array}[c]  00.2 \\\\ 0.2 \\end{array}\\right],   \\\\\n",
    "    \\ C_x = \\text{diag}(0.5, 0.25),  \\\\\n",
    "    \\ \\tilde{q} = [0.1], \\ C_q = [0.25].\n",
    "$$\n",
    "\n",
    "## Things to play with\n",
    "\n",
    "* Try changing the `x_prior` in the code to something other than $[0.2 \\ 0.2]^\\top$ to make the prior guess either better or worse. What happens?\n",
    "\n",
    "* Try playing with the `C_x` covariance to give the prior guess either more confidence (reduce the components) or less confidence (increase the components). What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup example and prior guess, prior prediction, and actual datum\n",
    "\n",
    "A = np.array([[2, -1]]) #map\n",
    "x_prior = np.array([[0.2, 0.2]]).transpose() #prior guess of mean\n",
    "q_obs = np.array([0.1]) #actual datum\n",
    "q_prior = np.dot(A,x_prior) #predicted datum using prior\n",
    "print('Prior Mean (x1,x2) =', *x_prior, 'maps to q =', *q_prior[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup all the covariances\n",
    "\n",
    "C_x = np.diag([0.5,0.25]) #prior covariance\n",
    "C_q = np.diag([0.25]) #data covariance\n",
    "C_A = np.dot(np.dot(A,C_x),A.transpose()) #the \"covariance of the map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the various weighted sum squares of errors (WSSE) given by \n",
    "# the data misfit, Tikhonov regularization, and unregularization terms. \n",
    "\n",
    "... # defining functions that depend on variables in the global namespace makes for bugs =/\n",
    "def data_misfit(x):\n",
    "    C_q_inv = np.linalg.inv(C_q)\n",
    "    q = np.dot(A,x)\n",
    "    WSSE = np.vdot(np.dot(C_q_inv,q-q_obs),q-q_obs) #weighted sum-squared error\n",
    "    return WSSE\n",
    "    \n",
    "def Tikhonov_reg(x):\n",
    "    C_x_inv = np.linalg.inv(C_x)\n",
    "    WSSE = np.vdot(np.dot(C_x_inv,x-x_prior),x-x_prior) #weighted sum-squared error\n",
    "    return WSSE\n",
    "\n",
    "def unregularize(x):\n",
    "    C_A_inv = np.linalg.inv(C_A)\n",
    "    q = np.dot(A,x)\n",
    "    WSSE = np.vdot(np.dot(C_A_inv,q-q_prior),q-q_prior) #weighted sum-squared error\n",
    "    return WSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize a portion of the input space R^2\n",
    "\n",
    "n = 101\n",
    "x1 = np.linspace(-0.5,0.5,n)\n",
    "x2 = x1\n",
    "x1,x2 = np.meshgrid(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all the WSSE terms\n",
    "\n",
    "WSSE = np.zeros((n,n))\n",
    "TSSE = np.zeros((n,n))\n",
    "USSE = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        WSSE[j,i] = data_misfit(np.array([[x1[j,i],x2[j,i]]]).transpose())\n",
    "        TSSE[j,i] = Tikhonov_reg(np.array([[x1[j,i],x2[j,i]]]).transpose())\n",
    "        USSE[j,i] = unregularize(np.array([[x1[j,i],x2[j,i]]]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(x1, x2, WSSE, cmap=cm.hot,\n",
    "                       linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(x1, x2, TSSE, cmap=cm.hot,\n",
    "                       linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(x1, x2, TSSE-USSE, cmap=cm.hot,\n",
    "                       linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(x1, x2, WSSE + TSSE, cmap=cm.hot,\n",
    "                       linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(x1, x2, WSSE + TSSE - USSE, cmap=cm.hot,\n",
    "                       linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reg_ind = np.argmin(WSSE+TSSE)\n",
    "print('MAP Tikonov point')\n",
    "print([x1.flatten()[x_reg_ind],x2.flatten()[x_reg_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unreg_ind = np.argmin(WSSE+TSSE-USSE)\n",
    "print('MAP CB point')\n",
    "print([x1.flatten()[x_unreg_ind],x2.flatten()[x_unreg_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Absolute error in prediction through Tikonov: ', \n",
    "      np.abs(0.1 - np.dot(A,[x1.flatten()[x_reg_ind],x2.flatten()[x_reg_ind]])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Absolute error in prediction through CB: ', \n",
    "      np.abs(0.1 - np.dot(A,[x1.flatten()[x_unreg_ind],x2.flatten()[x_unreg_ind]])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x1.flatten()[x_reg_ind],x2.flatten()[x_reg_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(n=101, data_cov_const=0.25, prior_x1=0.2, prior_x2=0.2, sigma_x1=0.5, sigma_x2=0.5):\n",
    "    # Discretize a portion of the input space R^2\n",
    "    # copied troy's code into here. \n",
    "    # Setup example and prior guess, prior prediction, and actual datum\n",
    "\n",
    "    A = np.array([[2, -1]]) #map\n",
    "    x_prior = np.array([prior_x1, prior_x2]).reshape(-1,1) #prior guess of mean\n",
    "    \n",
    "    def data_misfit(x):\n",
    "        C_q_inv = np.linalg.inv(C_q)\n",
    "        q = np.dot(A,x)\n",
    "        WSSE = np.vdot(np.dot(C_q_inv,q-q_obs),q-q_obs) #weighted sum-squared error\n",
    "        return WSSE\n",
    "    \n",
    "    def Tikhonov_reg(x):\n",
    "        C_x_inv = np.linalg.inv(C_x)\n",
    "        WSSE = np.vdot(np.dot(C_x_inv,x-x_prior),x-x_prior) #weighted sum-squared error\n",
    "        return WSSE\n",
    "\n",
    "    def unregularize(x):\n",
    "        C_A_inv = np.linalg.inv(C_A)\n",
    "        q = np.dot(A,x)\n",
    "        WSSE = np.vdot(np.dot(C_A_inv,q-q_prior),q-q_prior) #weighted sum-squared error\n",
    "        return WSSE\n",
    "\n",
    "    q_obs = np.array([0.1]) #actual datum # leave fixed.\n",
    "    q_prior = np.dot(A,x_prior) #predicted datum using prior\n",
    "    print('Prior Mean (x1,x2) =', *x_prior, 'maps to q =', *q_prior[0])\n",
    "    # Setup all the covariances\n",
    "    prior_cov = [sigma_x1, sigma_x2]\n",
    "    C_x = np.diag(prior_cov) #prior covariance\n",
    "    C_q = np.diag([data_cov_const]) #data covariance\n",
    "    C_A = np.dot(np.dot(A,C_x),A.transpose()) #the \"covariance of the map\"\n",
    "    \n",
    "    x1 = np.linspace(-0.5, 0.5,n)\n",
    "    x2 = x1\n",
    "    x1,x2 = np.meshgrid(x1,x2)\n",
    "    # Compute all the WSSE terms\n",
    "\n",
    "    WSSE = np.zeros((n,n))\n",
    "    TSSE = np.zeros((n,n))\n",
    "    USSE = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            WSSE[j,i] = data_misfit(np.array([[x1[j,i],x2[j,i]]]).transpose())\n",
    "            TSSE[j,i] = Tikhonov_reg(np.array([[x1[j,i],x2[j,i]]]).transpose())\n",
    "            USSE[j,i] = unregularize(np.array([[x1[j,i],x2[j,i]]]).transpose())\n",
    "\n",
    "    x_reg_ind = np.argmin(WSSE+TSSE)\n",
    "    x_unreg_ind = np.argmin(WSSE+TSSE-USSE)\n",
    "\n",
    "    \n",
    "    print('Absolute error in prediction through Tikonov: ', \n",
    "      np.abs(0.1 - np.dot(A,[x1.flatten()[x_reg_ind],x2.flatten()[x_reg_ind]])[0]))\n",
    "\n",
    "    print('Absolute error in prediction through CB: ', \n",
    "      np.abs(0.1 - np.dot(A,[x1.flatten()[x_unreg_ind],x2.flatten()[x_unreg_ind]])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Mean (x1,x2) = [0.2] [0.2] maps to q = 0.2\n",
      "Absolute error in prediction through Tikonov:  0.010000000000000092\n",
      "Absolute error in prediction through CB:  8.326672684688674e-17\n"
     ]
    }
   ],
   "source": [
    "solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from  ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0f21c63cec402c80ed49a06cafed23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.05, continuous_update=False, description='data_cov_const', max=2.0, min=0.001, step=0.001), FloatSlider(value=0.2, continuous_update=False, description='prior_x1', max=2.0, min=-2.0), FloatSlider(value=0.2, continuous_update=False, description='prior_x2', max=2.0, min=-2.0), FloatSlider(value=0.25, continuous_update=False, description='sigma_x1', max=5.0, min=0.025, step=0.025), FloatSlider(value=0.25, continuous_update=False, description='sigma_x2', max=5.0, min=0.025, step=0.025), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.solve>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(solve, n = ipywidgets.fixed(101),\n",
    "                   data_cov_const=ipywidgets.FloatSlider(value=0.05, min=0.001, max=2, step=0.001, continuous_update=False),\n",
    "                   sigma_x1=ipywidgets.FloatSlider(value=0.25, min=0.025, max=5, step=0.025, continuous_update=False), \n",
    "                   sigma_x2=ipywidgets.FloatSlider(value=0.25, min=0.025, max=5, step=0.025,  continuous_update=False), \n",
    "                   prior_x1=ipywidgets.FloatSlider(value=0.2, min=-2, max=2, continuous_update=False), \n",
    "                   prior_x2=ipywidgets.FloatSlider(value=0.2, min=-2, max=2, continuous_update=False)\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when the prior approaches a uniform (sigma large relative to the bounds on the space under consideration), the two techniques produce the same answer.\n",
    "- even a confident prior that is correct can lead to errors in the answer using Tikonov without un-regularization\n",
    "- more confidence in your data does lead to more accuracy with plain Tikonov, and after a certain amount of uncertainty in the datum, the prior is essentially informing us completely, so we converge towards some error due to the prior having some small variance and the coarseness of our mesh\n",
    "- if the prior and data are both uniformative, Tikonov gives a better approximation of the datum than if your prior was a tight gaussian distribution centered at the true data value.\n",
    "\n",
    "- important to note is that the solution (estimate of the data) given by the Consisteny Bayesian Formulation is not subject to these same sensitivities. It remains accurate, even tolerating very uncertain priors and data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
